Key topics involved:
- Multi head attention -> optimize the softmax operations
- Flash attention -> How can we block the matrix multiplications
- Make softmax safe -> Prevent exploding exponents
- Online softmax -> Heck, we proved it using induction
- Block matmuls -> Resulting in blocks of matrices, not scalars   
